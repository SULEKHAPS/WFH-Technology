{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b9c513b-6a01-40af-9948-e6e46c2b1d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 8.3 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.6/12.8 MB 8.9 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 7.6 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 8.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 8.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 8.4 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "631859b1-79b0-4cc7-9bad-5e8dc738f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Import libraries for Part A\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36f81adc-8cb4-4561-86e2-d4bd66314e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12aef54b-2a56-4673-a58e-943d92ca7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TaskExtractor:\n",
    "    def __init__(self):\n",
    "        # Expanded action verbs with more task-related terms\n",
    "        self.action_verbs = set([\n",
    "            'need', 'must', 'should', 'have to', 'has to', 'required',\n",
    "            'schedule', 'complete', 'review', 'submit', 'prepare',\n",
    "            'create', 'update', 'finish', 'deliver', 'implement',\n",
    "            'organize', 'coordinate', 'develop', 'ensure', 'plan'\n",
    "        ])\n",
    "        \n",
    "        # Expanded time indicators\n",
    "        self.time_indicators = set([\n",
    "            'today', 'tomorrow', 'tonight', 'next', 'by', 'before', 'after',\n",
    "            'monday', 'tuesday', 'wednesday', 'thursday', 'friday',\n",
    "            'saturday', 'sunday', 'week', 'month', 'asap', 'soon',\n",
    "            'morning', 'afternoon', 'evening', 'noon', 'midnight'\n",
    "        ])\n",
    "        \n",
    "        # Task categories with associated keywords\n",
    "        self.categories = {\n",
    "            'Work': ['report', 'project', 'document', 'review', 'develop', 'implement'],\n",
    "            'Meeting': ['meet', 'schedule', 'discuss', 'call', 'presentation'],\n",
    "            'Administrative': ['submit', 'file', 'organize', 'update', 'process'],\n",
    "            'Personal': ['buy', 'get', 'bring', 'clean', 'pick'],\n",
    "            'Deadline': ['urgent', 'asap', 'immediately', 'priority']\n",
    "        }\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess the input text with improved error handling.\"\"\"\n",
    "        try:\n",
    "            # Remove extra whitespace and normalize\n",
    "            text = re.sub(r'\\s+', ' ', text.strip())\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            # Extract meaningful sentences and filter out noise\n",
    "            sentences = []\n",
    "            for sent in doc.sents:\n",
    "                cleaned_sent = sent.text.strip()\n",
    "                if (len(cleaned_sent.split()) >= 3 and  # Minimum word requirement\n",
    "                    any(token.pos_ in ['VERB', 'AUX'] for token in sent)):  # Contains verb\n",
    "                    sentences.append(cleaned_sent)\n",
    "            \n",
    "            return sentences\n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def is_task(self, sentence):\n",
    "        \"\"\"Enhanced task detection with multiple heuristics.\"\"\"\n",
    "        if not sentence:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            doc = nlp(sentence.lower())\n",
    "            \n",
    "            # Multiple task indicators\n",
    "            indicators = {\n",
    "                'has_action_verb': any(token.lemma_ in self.action_verbs for token in doc),\n",
    "                'starts_with_verb': doc[0].pos_ == 'VERB' if len(doc) > 0 else False,\n",
    "                'has_future_tense': any(token.dep_ == 'aux' and token.lemma_ in ['will', 'shall'] \n",
    "                                      for token in doc),\n",
    "                'has_modal_verb': any(token.dep_ == 'aux' and token.lemma_ in ['must', 'should', 'need'] \n",
    "                                    for token in doc),\n",
    "                'has_to_infinitive': any(token.dep_ == 'aux' and token.head.lemma_ == 'have' \n",
    "                                       for token in doc),\n",
    "                'has_deadline': any(word in sentence.lower() for word in self.time_indicators)\n",
    "            }\n",
    "            \n",
    "            # Task confidence score based on indicators\n",
    "            confidence_score = sum(indicators.values())\n",
    "            return confidence_score >= 1  # Require at least one strong indicator\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in task detection: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def extract_entity_and_deadline(self, sentence):\n",
    "        \"\"\"Improved entity and deadline extraction.\"\"\"\n",
    "        try:\n",
    "            doc = nlp(sentence)\n",
    "            \n",
    "            # Entity extraction\n",
    "            entities = []\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PERSON':\n",
    "                    entities.append(ent.text)\n",
    "            \n",
    "            # Look for subject if no named entity found\n",
    "            if not entities:\n",
    "                for token in doc:\n",
    "                    if token.dep_ == 'nsubj' and token.pos_ == 'PROPN':\n",
    "                        entities.append(token.text)\n",
    "            \n",
    "            # Deadline extraction with context\n",
    "            deadline = None\n",
    "            doc_lower = sentence.lower()\n",
    "            \n",
    "            # Pattern matching for deadline phrases\n",
    "            deadline_patterns = [\n",
    "                r'by\\s+([\\w\\s]+)',\n",
    "                r'before\\s+([\\w\\s]+)',\n",
    "                r'due\\s+(?:by|on)?\\s+([\\w\\s]+)',\n",
    "                r'until\\s+([\\w\\s]+)'\n",
    "            ]\n",
    "            \n",
    "            for pattern in deadline_patterns:\n",
    "                match = re.search(pattern, doc_lower)\n",
    "                if match:\n",
    "                    deadline = match.group(1).strip()\n",
    "                    break\n",
    "            \n",
    "            # Fallback to time indicators\n",
    "            if not deadline:\n",
    "                for token in doc:\n",
    "                    if token.text.lower() in self.time_indicators:\n",
    "                        deadline = ' '.join([t.text for t in token.subtree])\n",
    "                        break\n",
    "            \n",
    "            return entities[0] if entities else None, deadline\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in entity/deadline extraction: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def determine_category(self, task_text):\n",
    "        \"\"\"Determine task category based on keyword matching.\"\"\"\n",
    "        task_lower = task_text.lower()\n",
    "        category_scores = {cat: 0 for cat in self.categories}\n",
    "        \n",
    "        for category, keywords in self.categories.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in task_lower:\n",
    "                    category_scores[category] += 1\n",
    "        \n",
    "        # Return category with highest score, default to 'Work'\n",
    "        max_score = max(category_scores.values())\n",
    "        if max_score == 0:\n",
    "            return 'Work'\n",
    "        return max(category_scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    def extract_tasks(self, text):\n",
    "        \"\"\"Main function to extract and categorize tasks with improved structure.\"\"\"\n",
    "        try:\n",
    "            sentences = self.preprocess_text(text)\n",
    "            tasks = []\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if self.is_task(sentence):\n",
    "                    entity, deadline = self.extract_entity_and_deadline(sentence)\n",
    "                    category = self.determine_category(sentence)\n",
    "                    \n",
    "                    task = {\n",
    "                        'task': sentence,\n",
    "                        'assignee': entity,\n",
    "                        'deadline': deadline,\n",
    "                        'category': category,\n",
    "                        'priority': 'High' if any(urgent in sentence.lower() \n",
    "                                                for urgent in ['urgent', 'asap', 'immediately']) \n",
    "                                  else 'Normal'\n",
    "                    }\n",
    "                    tasks.append(task)\n",
    "            \n",
    "            return tasks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in task extraction: {str(e)}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a7762ad-1110-41fa-b448-3b6face186eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Tasks:\n",
      "\n",
      "Task: Rahul has to buy snacks for all of us by 5pm today.\n",
      "Assignee: Rahul\n",
      "Deadline: 5pm today\n",
      "Category: Personal\n",
      "Priority: Normal\n",
      "\n",
      "Task: Sarah needs to review the quarterly report before Monday.\n",
      "Assignee: Sarah\n",
      "Deadline: monday\n",
      "Category: Work\n",
      "Priority: Normal\n",
      "\n",
      "Task: James must schedule the team meeting for next week.\n",
      "Assignee: James\n",
      "Deadline: next\n",
      "Category: Meeting\n",
      "Priority: Normal\n",
      "\n",
      "Task: We should complete the project documentation by Friday.\n",
      "Assignee: None\n",
      "Deadline: friday\n",
      "Category: Work\n",
      "Priority: Normal\n",
      "\n",
      "Task: Please submit the expense reports urgently.\n",
      "Assignee: None\n",
      "Deadline: None\n",
      "Category: Work\n",
      "Priority: High\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Test the implementation\n",
    "test_text = \"\"\"\n",
    "Rahul has to buy snacks for all of us by 5pm today. \n",
    "Sarah needs to review the quarterly report before Monday. \n",
    "The weather is nice outside. \n",
    "James must schedule the team meeting for next week.\n",
    "We should complete the project documentation by Friday.\n",
    "Please submit the expense reports urgently.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize and test\n",
    "extractor = TaskExtractor()\n",
    "tasks = extractor.extract_tasks(test_text)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nExtracted Tasks:\")\n",
    "for task in tasks:\n",
    "    print(f\"\\nTask: {task['task']}\")\n",
    "    print(f\"Assignee: {task['assignee']}\")\n",
    "    print(f\"Deadline: {task['deadline']}\")\n",
    "    print(f\"Category: {task['category']}\")\n",
    "    print(f\"Priority: {task['priority']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
